{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install sentencepiece\n",
        "!pip3 install transformers\n",
        "!pip3 install imbalanced-learn\n",
        "!pip3 install pytorch-metric-learning"
      ],
      "metadata": {
        "id": "UIEKCH5vK_sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "G8v-uxSuityz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6ZqCnUCC-aj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from pytorch_metric_learning import losses as loss_fun\n",
        "from pytorch_metric_learning.distances import CosineSimilarity\n",
        "from pytorch_metric_learning.reducers import ThresholdReducer\n",
        "from pytorch_metric_learning.regularizers import LpRegularizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from pylab import rcParams\n",
        "\n",
        "# Seed\n",
        "seed = 666\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "K5zvDAeMDknK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoBERTa_dataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        super(RoBERTa_dataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.labels[idx], self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "class ImdbDataset(Dataset):\n",
        "\n",
        "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item])\n",
        "        target = self.targets[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids = pad_sequences(encoding['input_ids'], maxlen=self.max_len, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        input_ids = input_ids.astype(dtype = 'int64')\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "\n",
        "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=self.max_len, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "        return {\n",
        "        'review_text': review,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask.flatten(),\n",
        "        'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "iVwvMakNKXqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampler(object):\n",
        "    \"\"\"Base class for all Samplers.\n",
        "    Every Sampler subclass has to provide an __iter__ method, providing a way\n",
        "    to iterate over indices of dataset elements, and a __len__ method that\n",
        "    returns the length of the returned iterators.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_source):\n",
        "        pass\n",
        "\n",
        "    def __iter__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class StratifiedSampler(Sampler):\n",
        "    \"\"\"Stratified Sampling\n",
        "    Provides equal representation of target classes in each batch\n",
        "    \"\"\"\n",
        "    def __init__(self, class_vector, batch_size):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        class_vector : torch tensor\n",
        "            a vector of class labels\n",
        "        batch_size : integer\n",
        "            batch_size\n",
        "        \"\"\"\n",
        "        self.n_splits = int(class_vector.size(0) / batch_size)\n",
        "        self.class_vector = class_vector\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        try:\n",
        "            from sklearn.model_selection import StratifiedShuffleSplit\n",
        "        except:\n",
        "            print('Need scikit-learn for this functionality')\n",
        "        import numpy as np\n",
        "\n",
        "        s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5)\n",
        "        X = torch.randn(self.class_vector.size(0),2).numpy()\n",
        "        y = self.class_vector.numpy()\n",
        "        s.get_n_splits(X, y)\n",
        "\n",
        "        train_index, test_index = next(s.split(X, y))\n",
        "        return np.hstack([train_index, test_index])\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_vector)"
      ],
      "metadata": {
        "id": "GMhJOg99vGeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getloader_undersample(X,y_digit,train_stratified,aspect,tokenizer,max_seq_lenght,max_size,drop_last=True):\n",
        "    pos_train = []\n",
        "    neg_train = []\n",
        "    pos_test = []\n",
        "    neg_test = []\n",
        "    for data in train_stratified:\n",
        "        if y_digit[data]==1:\n",
        "            pos_train.append(X[data])\n",
        "        else:\n",
        "            neg_train.append(X[data])\n",
        "\n",
        "    neg_train = random.sample(neg_train,len(pos_train))\n",
        "\n",
        "\n",
        "    tartget_data = pos_train+neg_train\n",
        "    tartget_label = [1]*len(pos_train)+[0]*len(neg_train)\n",
        "\n",
        "\n",
        "    dataset = ImdbDataset(tartget_data,tartget_label,tokenizer,max_len=max_size)\n",
        "    return DataLoader(dataset, batch_size=32, shuffle=True, num_workers=1, pin_memory=True, drop_last=drop_last)\n",
        "\n",
        "def getloader(X,y_digit,train_stratified,aspect,tokenizer,max_seq_lenght,max_size,drop_last=True):\n",
        "    pos_train = []\n",
        "    neg_train = []\n",
        "    pos_test = []\n",
        "    neg_test = []\n",
        "    for data in train_stratified:\n",
        "        if y_digit[data]==1:\n",
        "            pos_train.append(X[data])\n",
        "        else:\n",
        "            neg_train.append(X[data])\n",
        "\n",
        "    tartget_data = pos_train+neg_train\n",
        "    tartget_label = [1]*len(pos_train)+[0]*len(neg_train)\n",
        "\n",
        "\n",
        "    dataset = ImdbDataset(tartget_data,tartget_label,tokenizer,max_len=max_size)\n",
        "    return DataLoader(dataset, batch_size=32, shuffle=True, num_workers=1, pin_memory=True, drop_last=drop_last)\n",
        "\n",
        "\n",
        "def get_train_valid_Bert_undersample(samplestrategy,X,y_digit,aspect,train_stratified,val_stratified,x2, tokenizer, max_seq_lenght):\n",
        "    if samplestrategy == True:\n",
        "        loader_train = getloader_undersample(X,y_digit,train_stratified,aspect,tokenizer,max_seq_lenght,max_size=max_seq_lenght)\n",
        "    else:\n",
        "        loader_train = getloader(X,y_digit,train_stratified,aspect,tokenizer,max_seq_lenght,max_size=max_seq_lenght)\n",
        "\n",
        "    loader_test = getloader(X,y_digit,x2,aspect,tokenizer,max_seq_lenght,max_size=max_seq_lenght)\n",
        "\n",
        "    loader_valid = getloader(X,y_digit,val_stratified,aspect,tokenizer,max_seq_lenght,max_size=max_seq_lenght)\n",
        "    return loader_train, loader_valid, loader_test\n",
        "\n",
        "def getloader_all(X,y_digit,tokenizer,max_size,drop_last=True):\n",
        "    pos_train = []\n",
        "    neg_train = []\n",
        "    pos_test = []\n",
        "    neg_test = []\n",
        "    for data in range(len(y_digit)):\n",
        "        print(data)\n",
        "        if y_digit[data]==1:\n",
        "            pos_train.append(X[data])\n",
        "        else:\n",
        "            neg_train.append(X[data])\n",
        "\n",
        "    tartget_data = pos_train+neg_train\n",
        "    tartget_label = [1]*len(pos_train)+[0]*len(neg_train)\n",
        "\n",
        "    dataset = ImdbDataset(tartget_data,tartget_label,tokenizer,max_len=max_size)\n",
        "    return DataLoader(dataset, batch_size=16, shuffle=True, num_workers=1, pin_memory=True, drop_last=drop_last)"
      ],
      "metadata": {
        "id": "l17iPUewK6TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeoutput(info):\n",
        "      mylog = open('record_tmp_2.log',mode = 'a', encoding='utf_8')\n",
        "      print(info,file=mylog)\n",
        "      mylog.close()\n",
        "\n",
        "def translate(label,aspect):\n",
        "      label_all = []\n",
        "      count = 0\n",
        "\n",
        "      for i in range(len(label)):\n",
        "            if aspect in label[i]:\n",
        "                  label_all.append(1)\n",
        "                  count+=1\n",
        "            else:\n",
        "                  label_all.append(0)\n",
        "      return label_all,count\n",
        "\n",
        "#分层抽样   x1是数据的标号，x1里拆分出10%的验证集，ydigit是所有数据的label\n",
        "def returnlabel(x1,y_digit):\n",
        "      count=0\n",
        "      count_negative = 0\n",
        "      y_pos = []\n",
        "      y_neg = []\n",
        "      for x in x1:\n",
        "            if y_digit[x]==1:\n",
        "                  count+=1\n",
        "                  y_pos.append(x)\n",
        "            else:\n",
        "                  count_negative+=1\n",
        "                  y_neg.append(x)\n",
        "      # ratio = float(count/(count+count_negative))\n",
        "      # 1/10正例的个数，四舍五入\n",
        "      pos_train,pos_val = train_test_split(y_pos,train_size=0.9)\n",
        "      neg_train,neg_val = train_test_split(y_neg,train_size=0.9)\n",
        "\n",
        "\n",
        "      return pos_train+neg_train,pos_val+neg_val\n",
        "\n",
        "def aspect_wise_data(x1,y_digit,undersample):\n",
        "    pos_train = []\n",
        "    neg_train = []\n",
        "    pos_test = []\n",
        "    neg_test = []\n",
        "\n",
        "    count = 0\n",
        "    for data in x1:\n",
        "      if y_digit[data] == 1:\n",
        "        count = count+1\n",
        "\n",
        "    if count < 100:\n",
        "      for data in x1:\n",
        "        if y_digit[data]==1:\n",
        "            pos_train.append(data)\n",
        "            pos_train.append(data)\n",
        "        else:\n",
        "            neg_train.append(data)\n",
        "    else:\n",
        "      for data in x1:\n",
        "        if y_digit[data]==1:\n",
        "            pos_train.append(data)\n",
        "        else:\n",
        "            neg_train.append(data)\n",
        "\n",
        "    if undersample == True:\n",
        "      neg_train = random.sample(neg_train,len(pos_train))\n",
        "      btch_size=8\n",
        "    else:\n",
        "      pos_train = random.sample(pos_train, len(pos_train))\n",
        "      btch_size=32\n",
        "\n",
        "    tartget_data = pos_train+neg_train\n",
        "    tartget_label = [1]*len(pos_train)+[0]*len(neg_train)\n",
        "\n",
        "    sampler = StratifiedSampler(class_vector=torch.from_numpy(np.array(tartget_label)), batch_size=btch_size)\n",
        "    dataset = ImdbDataset(tartget_data,tartget_label,tokenizer,max_len=160)\n",
        "    return DataLoader(dataset, batch_size=btch_size, sampler=sampler, num_workers=1, pin_memory=True, drop_last=True)"
      ],
      "metadata": {
        "id": "W7CT47-fMYqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def supervisedContrastiveTraining(model, train_dataloader, epochs, device, optimizer, scheduler):\n",
        "\n",
        "  #change here for using different loss function from https://kevinmusgrave.github.io/pytorch-metric-learning/\n",
        "  loss_function = loss_fun.SupConLoss(temperature=0.1,embedding_regularizer = LpRegularizer())\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    model.zero_grad()\n",
        "    model.train()\n",
        "\n",
        "    for d in train_dataloader:\n",
        "      input_ids = d[\"input_ids\"].reshape(32, 160).to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
        "\n",
        "      hidden_states = outputs[0]\n",
        "\n",
        "      supcon_fea_cls = F.normalize(hidden_states[:,0,:],dim=1)\n",
        "      #supcon_fea_cls = F.normalize(hidden_states[-1][:,0,:],dim=1)\n",
        "\n",
        "      loss = loss_function(supcon_fea_cls, targets)\n",
        "      if not torch.isnan(loss):\n",
        "        losses.append(loss.item())\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    print('Contrastive Loss Mean: ', np.mean(losses))"
      ],
      "metadata": {
        "id": "iP5VaStuHHwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
        "  model.zero_grad()\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  acc = 0\n",
        "  counter = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].reshape(32,160).to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        _, prediction = torch.max(outputs[1], dim=1)\n",
        "        targets = targets.cpu().detach().numpy()\n",
        "        prediction = prediction.cpu().detach().numpy()\n",
        "        try:\n",
        "              accuracy = metrics.accuracy_score(targets.tolist(), prediction.tolist())\n",
        "        except:\n",
        "              print(targets)\n",
        "              print(prediction)\n",
        "        acc += accuracy\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        counter = counter + 1\n",
        "  return acc / counter, np.mean(losses)"
      ],
      "metadata": {
        "id": "MiMrsRTLLYso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  f1_sum = 0\n",
        "  counter = 0\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "              input_ids = d[\"input_ids\"].reshape(32,160).to(device)\n",
        "              attention_mask = d[\"attention_mask\"].to(device)\n",
        "              targets = d[\"targets\"].to(device)\n",
        "\n",
        "              outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n",
        "              loss = outputs[0]\n",
        "              logits = outputs[1]\n",
        "\n",
        "              _, prediction = torch.max(outputs[1], dim=1)\n",
        "              targets = targets.cpu().detach().numpy()\n",
        "              prediction = prediction.cpu().detach().numpy()\n",
        "              f1 = metrics.f1_score(targets, prediction)\n",
        "\n",
        "              f1_sum += f1\n",
        "              y_pred.extend(prediction)\n",
        "              y_true.extend(targets.tolist())\n",
        "              losses.append(loss.item())\n",
        "              counter += 1\n",
        "\n",
        "  return metrics.f1_score(y_true, y_pred,average='weighted'), np.mean(losses), classification_report(y_true, y_pred, labels=[0,1], digits=4),metrics.precision_score(y_true, y_pred,average='weighted'),metrics.recall_score(y_true, y_pred,average='weighted'), metrics.matthews_corrcoef(y_true, y_pred), metrics.roc_auc_score(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "YxXB2wj6MMFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRE_TRAINED_MODEL_NAME = \"jeniya/BERTOverflow\"\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "id": "xzLxIp3yOft7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data load\n",
        "datapath = '/content/BenchmarkUddinSO-ConsoliatedAspectSentiment.csv'\n",
        "data = pd.read_csv(datapath,sep=',')\n",
        "X = data['sent'] # data\n",
        "y = data['codes'] # label\n",
        "\n",
        "k = 10 # 10 fold cross validation\n",
        "kf = StratifiedKFold(n_splits=k, random_state=None)"
      ],
      "metadata": {
        "id": "sKkXKAxzOqtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aspect_list = ['Performance','Usability','Security','Community','Compatibility','Portability','Documentation','Bug','Legal','OnlySentiment','Other']\n",
        "\n",
        "aspects = ['Performance']\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "learn_rate_list = [5e-5]\n",
        "undersample = [False]"
      ],
      "metadata": {
        "id": "AkWdPPizSmaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_training(X, con_y_digit):\n",
        "  print(\"Contrastive Learning:\")\n",
        "\n",
        "  model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, output_hidden_states=True)\n",
        "  model = model.to(device)\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "  optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}]\n",
        "  optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learn_rate_list[0])\n",
        "\n",
        "  con_train = aspect_wise_data(X, con_y_digit, False)\n",
        "  total_steps = len(con_train) * EPOCHS\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
        "\n",
        "  supervisedContrastiveTraining(model, con_train, EPOCHS, device, optimizer, scheduler)\n",
        "  return model"
      ],
      "metadata": {
        "id": "gnnTMK8ISuVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Aspect_Detection(aspect, learn_rate):\n",
        "\n",
        "  dataframe = pd.DataFrame()\n",
        "  avg_df = pd.DataFrame()\n",
        "\n",
        "  best_pre=[]\n",
        "  best_re=[]\n",
        "  best_f=[]\n",
        "  best_mcc=[]\n",
        "  best_auc=[]\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  y_digit, true_label_number = translate(y, aspect)\n",
        "  print(\"True Label Number: \", true_label_number)\n",
        "\n",
        "  for x1,x2 in kf.split(X,y_digit):\n",
        "    print(\"inside k-fold iteration: \", count)\n",
        "    print(len(x1))\n",
        "    print(len(x2))\n",
        "\n",
        "    train_stratified, val_stratified = returnlabel(x1, y_digit)\n",
        "    best_F1 = -1\n",
        "    train_iter, val_iter, test_iter = get_train_valid_Bert_undersample(False, X, y_digit, aspect, train_stratified, val_stratified, x2, tokenizer, max_seq_lenght=160)\n",
        "    print(\"training data size: \", len(train_iter))\n",
        "    print(\"testing data size: \", len(test_iter))\n",
        "\n",
        "    '''uncomment these when fine-tuning with contrastive learning'''\n",
        "    # model = contrastive_training(train_stratified, y_digit)\n",
        "    # model.save_pretrained('/content/contrastive/')\n",
        "\n",
        "    best_epoch_F1 = -1\n",
        "\n",
        "    try:\n",
        "      fine_model =  BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 2) #replace PRE_TRAINED_MODEL_NAME with /content/contrastive/ for CL\n",
        "    except:\n",
        "      time.sleep(15)\n",
        "      fine_model =  BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 2) #replace PRE_TRAINED_MODEL_NAME with /content/contrastive/ for CL\n",
        "\n",
        "    fine_model.to(device)\n",
        "    fine_param_optimizer = list(fine_model.named_parameters())\n",
        "    fine_no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    fine_optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in fine_param_optimizer if not any(nd in n for nd in fine_no_decay)], 'weight_decay': 0.01},\n",
        "      {'params': [p for n, p in fine_param_optimizer if any(nd in n for nd in fine_no_decay)], 'weight_decay':0.0}]\n",
        "    fine_optimizer = torch.optim.AdamW(fine_optimizer_grouped_parameters, lr=learn_rate)\n",
        "    fine_total_steps = len(train_iter) * EPOCHS\n",
        "    fine_scheduler = get_linear_schedule_with_warmup(fine_optimizer,num_warmup_steps=0,num_training_steps=fine_total_steps)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "      print('-' * 10)\n",
        "\n",
        "      #fine tuning\n",
        "      print(\"Fine tuning\")\n",
        "      train_acc, train_loss = train_epoch(\n",
        "            fine_model,\n",
        "            train_iter,\n",
        "            fine_optimizer,\n",
        "            device,\n",
        "            fine_scheduler,\n",
        "            len(train_iter)\n",
        "      )\n",
        "      print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
        "      val_F_1, val_loss, metrics_all, val_precision, val_recall, _, _ = eval_model(\n",
        "            fine_model,\n",
        "            val_iter,\n",
        "            device,\n",
        "            len(val_iter)\n",
        "      )\n",
        "      print(f'Val loss {val_loss} Val F-1 {val_F_1}')\n",
        "      if val_F_1 > best_epoch_F1:\n",
        "            best_epoch_F1 = val_F_1\n",
        "            fine_model.save_pretrained('/content/best_model/')\n",
        "\n",
        "    print(\"Epoch End--->\")\n",
        "    best_model = BertForSequenceClassification.from_pretrained('/content/best_model/', num_labels = 2)\n",
        "    best_model.to(device)\n",
        "    test_F_1, test_loss, metrics_all, test_precision, test_recall, test_mcc, test_auc = eval_model(\n",
        "                best_model,\n",
        "                test_iter,\n",
        "                device,\n",
        "                len(test_iter)\n",
        "          )\n",
        "    if test_F_1 > best_F1:\n",
        "          best_F1 = test_F_1\n",
        "          best_metrics = metrics_all\n",
        "          best_precision = test_precision\n",
        "          best_recall = test_recall\n",
        "          best_learn_rate = learn_rate\n",
        "          best_t_mcc = test_mcc\n",
        "          best_t_auc = test_auc\n",
        "          best_model.save_pretrained('/content/test_model/')\n",
        "\n",
        "    #del model\n",
        "    del fine_model\n",
        "    del best_model\n",
        "\n",
        "    best_pre.append(best_precision)\n",
        "    best_re.append(best_recall)\n",
        "    best_f.append(best_F1)\n",
        "    best_mcc.append(best_t_mcc)\n",
        "    best_auc.append(best_t_auc)\n",
        "\n",
        "    print(best_metrics)\n",
        "    print(\"MSCC: \", best_t_mcc, \" AUC: \",best_t_auc)\n",
        "    count += 1\n",
        "\n",
        "  dataframe[\"aspect\"] = [aspect]*10\n",
        "  dataframe[\"learn_rate\"] = [best_learn_rate]*10\n",
        "  dataframe['sample strategy'] = [False]*10\n",
        "  dataframe['best_precision'] = best_pre\n",
        "  dataframe['best recall']=best_re\n",
        "  dataframe['best F-1']=best_f\n",
        "  dataframe['best MCC']=best_mcc\n",
        "  dataframe['best AUC']=best_auc\n",
        "\n",
        "  avg_df['aspect'] = [aspect]\n",
        "  avg_df['avg_precision'] = [np.mean(best_pre)]\n",
        "  avg_df['avg_recall'] = [np.mean(best_re)]\n",
        "  avg_df['avg_f1'] = [np.mean(best_f)]\n",
        "  avg_df['avg_mcc'] = [np.mean(best_mcc)]\n",
        "  avg_df['avg_auc'] = [np.mean(best_auc)]\n",
        "\n",
        "  print(dataframe.head())\n",
        "\n",
        "  dataframe.to_csv('Detail result.csv', index=False)\n",
        "  avg_df.to_csv('Average result.csv', index=False)"
      ],
      "metadata": {
        "id": "xTJ9mtiujOtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Aspect_Detection(aspects[0], learn_rate_list[0])"
      ],
      "metadata": {
        "id": "lvRV63s-wT2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}